{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRAUD DETECTION on ONLINE TRANSACTIONS \n",
    "\n",
    "\n",
    "### Project Overview\n",
    "In this project I trained several models to detect fraud transactions. I have started 5 baseline models. Those are, LogisticRegression, KNeighborsClassifier, RandomForestClassifier, XGBClassifier, SupportVectorMachine Classifier. I continued to optimize top two models based on their train and test accuracy result. XGBoost and RandomForest Models. I have done five iterations including grid search on hyperparameters, balancing the labels by SMOTE and subsampling from the original dataset. Both RandomForest and XGBoost model had over 99% accuracy on the data that includes all frauds and some random safe data. The data was still imbalanced so I did SMOTE over this dataset as well. At the end of those iterations, **XGBoost model had 99% Accuracy Score** on both train and test sets. Accuracy score was calculated by computing Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "\n",
    "\n",
    "![](bank.png)\n",
    "### Project Steps\n",
    "\n",
    "- 1.Loading Data and EDA\n",
    "- 2.Feature Engineering\n",
    "- 3.Machine Learning\n",
    "    - 3.1. Baseline Models\n",
    "    - 3.2. Grid Search for Best Hyper-parameter\n",
    "    - 3.3. Dealing with Unbalanced Data\n",
    "        - 3.3.1. Balancing Data via Resambling with SMOTE\n",
    "        - 3.3.2. Subsampling Data from the Original Dataset\n",
    "        - 3.3.3 Performing SMOTE on the New Data\n",
    "- 4.Machine Learning Pipeline\n",
    "- 5.Feature Importance\n",
    "- 6.Conclusion\n",
    "- 7.Future Works\n",
    "\n",
    "### Data\n",
    "I used Kaggle's Paysim dataset. It simulates mobile money transactions based on a sample of real transactions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, who is the provider of the mobile financial service which is currently running in more than 14 countries all around the world.\n",
    "\n",
    "https://www.kaggle.com/ntnu-testimon/paysim1\n",
    "\n",
    "**Dataset has fillowing columns:**\n",
    "\n",
    "**step** - maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation).\n",
    "\n",
    "**type** - CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.\n",
    "\n",
    "**amount** - amount of the transaction in local currency.\n",
    "\n",
    "**nameOrig** - customer who started the transaction\n",
    "\n",
    "**oldbalanceOrg** - initial balance before the transaction\n",
    "\n",
    "**newbalanceOrig** - new balance after the transaction\n",
    "\n",
    "**nameDest** - customer who is the recipient of the transaction\n",
    "\n",
    "**oldbalanceDest** - initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants).\n",
    "\n",
    "**newbalanceDest** - new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants).\n",
    "\n",
    "**isFraud** - This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.\n",
    "\n",
    "**isFlaggedFraud** - The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Data and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from numpy import * \n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns #for visualization\n",
    "import matplotlib.pyplot as plt #for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load data \n",
    "data=pd.read_csv('paysim.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA with full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shape of the dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there is anu null values\n",
    "data.isna().sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicate values\n",
    "data.duplicated(keep='first').any()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are no duplicate rows, so we do not need to worry about duplicated data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of all Transactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of the frequency of all transactions\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.distplot(data.step)\n",
    "plt.title('Distribution of Transactions over the Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distrubition plot shows number of transactions occured each hour (step). There are drastic changes in the number of transactions happens time to time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the data by the labels\n",
    "I will filter the data by the labels and examine two groups compairing each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data by the labels. Safe and Fraud transaction\n",
    "safe = data[data['isFraud']==0]\n",
    "fraud = data[data['isFraud']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the frequency of the transactions for each class on the same plot.\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.distplot(safe.step, label=\"Safe Transaction\")\n",
    "sns.distplot(fraud.step, label='Fraud Transaction')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.title('Distribution of Transactions over the Time')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventhough safe transactions slows down in 3rd and 4th day and after 16th day of the month, fraud transactions happens at a steady pace. Especially in the second half of the month there are much less safe transactions but number of fraud transactions does not decrease at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hourly Transaction Amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#just use small portion of data to scatterplot the transaction happens every hour and their amount. \n",
    "smalldata=data.sample(n=100000, random_state=1)\n",
    "smalldata=smalldata.sort_index()\n",
    "smalldata=smalldata.reset_index(drop=True)\n",
    "\n",
    "#plot the small data\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.ylim(0, 10000000)\n",
    "plt.title('Hourly Transaction Amounts')\n",
    "ax = sns.scatterplot(x=\"step\", y=\"amount\", hue=\"isFraud\",\n",
    "                     data=smalldata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot clearly shows that there is some sort of seasonality in the number of transaction during the day. We observe a pattern every 24 hours. we do not know what time of the day '0' represent here but we observe highest transactions clusters around the middle of 24 hour period. It mught be noon or mid day. Lets see if fraud transactions has that kind of pattern.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The hourly amount of al fraud transactions\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.ylim(0, 10000000)\n",
    "plt.title('Hourly Fraud Transaction Amounts')\n",
    "ax = sns.scatterplot(x=\"step\", y=\"amount\", color='orange',\n",
    "                     data=fraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraud transactions does not show that significant pattern like safe ones in terms of number of accurance. They happen every hour almast in the same frequency. There are more fraud transactions in low amounts and less in high amount. But the pattern does not change time to time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction Amount Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safe transactions amount distribution plot\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.ylim(0, 0.0000000075)\n",
    "plt.title('Safe Transaction Amount Distribution')\n",
    "sns.distplot(safe.amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe.amount.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Safe transactions also more often in the low amounts . There is a peek in 1M dolar but above that the frequency decreases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud transactions amount distribution plot\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.title('Fraud Transaction Amount Distribution')\n",
    "sns.distplot(fraud.amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an interesting peak on 1M$. Lets see how many fraud transactions happens at 1M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraud transactions amount value counts\n",
    "fraud.amount.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are fraud transactions in $1M amount for 287 times. And this is the max amount of fraud transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fraud transaction boxplot for amount distribution\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.title('Fraud Transaction Amount Distribution')\n",
    "ax = sns.boxplot(x=fraud[\"amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud.amount.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the frauds happens below $400000 so lets check the average amount for those transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average amount for frauds below 400K\n",
    "fraud[fraud.amount<400000].amount.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Fraud transaction happens in a large range such as $119 to 10M. The Frequency distribution of Amount of money involved in Fraud transactions is Positively Skewed. Most of the fraud transactions are of Lesser amount. Majority of fraud transactions are lower than 1M. But in 1M there is an interesting increase similar to safe transactions. And that is also max amount in all fraud incidents. There are also some fraud labeled transaction that have 0 amount. This is strange. I want to see those instances, there are 16 of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking \"0\" in the fraud  amount\n",
    "fraud[fraud.amount==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are definetely not correct data. But it might have some sort of value such as creating some noise in the transaction traffic to make the real fraud not to be noticed. For that reason I will keep this data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking type of  safe transactions\n",
    "safe.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking type of fraud transactions\n",
    "fraud.type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraud activities only happens with transfer and cash_out transactions. Debit usage is very safe. It will be better to **use only Transfer and Cash_out transaction data** for our model since the other types has no fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate of Fraud Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proportion of number of frauds \n",
    "data.isFraud.value_counts()[1]/(data.isFraud.value_counts()[0]+data.isFraud.value_counts()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraud transactions are only 0.01% of the safe transactions. Target class is pretty skewed. It might be problem in the model but we will see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proportion of fraud amount\n",
    "fraud.amount.sum()/(safe.amount.sum()+fraud.amount.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total money was stolen is 0.1% of safe transaction amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### isFlaggedFraud column\n",
    "\n",
    "Lets examine the isFlaggedFraud column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value counts of isFlaggedFraud column\n",
    "data.isFlaggedFraud.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are only 16 positive value. lets see which ones they are. \n",
    "data[data.isFlaggedFraud==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All positive values in isFlaggedFraud are also positive on is Fraud Column. There is also inconsistancy in Origin and destination balances on theseinstances.May be that is why they were marked Fraud. It could be valuable information, I will keep it too.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First only get Transfer and Cash_out transaction data\n",
    "Since fraud transactions happens only in these type of transactions, I will use only that data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering only transfer and cash_out data\n",
    "data_by_type=data[data['type'].isin(['TRANSFER','CASH_OUT'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 100000 random samples from the filtered data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is too big to work with a machine learning algorithm. I will get random subsample from this dataframe just big enough to built a machine learning model. For such project 100000 instance would be good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsample data , get 100000 instances to train model\n",
    "df=data_by_type.sample(n=100000, random_state=1)\n",
    "df=df.sort_index()\n",
    "df=df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with name columns\n",
    "\n",
    "nameOrig and nameDest columns are supposed to be the names of the peeople. At this moment, they can not be used in machine learning model. But if there is any repeting transaction between two people that might me useful information for classifier.I can create a new column with numeric value with repeat info. Let me check. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if there is any repetes transaction in between two parties.\n",
    "list1=np.array(df.nameOrig)\n",
    "list2=np.array(df.nameDest)\n",
    "list3=list1+list2\n",
    "repeat=pd.DataFrame(list3, columns=['comb'])\n",
    "comb_cnt=repeat.comb.value_counts()\n",
    "comb_cnt.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there is no repeated transaction between 2 parties, each of them are unique. So, we can just drop these string columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary encoding the type column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can use get.dummies() but this faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the name columns\n",
    "df=df.drop(['nameOrig', 'nameDest'], axis=1)\n",
    "#Binary-encoding of labelled data in 'type'\n",
    "df.loc[df.type == 'CASH_OUT', 'type'] = 1\n",
    "df.loc[df.type == 'TRANSFER', 'type'] = 0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is something wrong with the balance information. Eventhough a transaction going on both old and new balance looks '0'.  But I will ignore it for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm  #import support vector machine classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # import train_test_split function\n",
    "from sklearn.linear_model import LogisticRegression # import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score # import accuracy metrics\n",
    "from sklearn.ensemble import RandomForestClassifier #import RandomForestClassifier\n",
    "import xgboost as xgb  \n",
    "from xgboost import  XGBClassifier  #import xgboost classifier \n",
    "from sklearn.neighbors import KNeighborsClassifier #import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV # import GridSearchCV\n",
    "# suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slice the target and features from the dataset\n",
    "features=df.drop('isFraud', axis=1)\n",
    "target =df.isFraud\n",
    "\n",
    "# split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Baseline Models\n",
    "\n",
    "First, I will run five classification madel with their default parameter to see how each one perform. I put all the classifers into a list and train them in a loop. ml_func function handles all train, evaluation and storing the performence metrics. Also, the data is highly unbalanced, the positive class (frauds) account for 0.01% of all transactions. So I will be measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General function to run classifier with default parameters to get baseline model\n",
    "def ml_func (algoritm):\n",
    "    #train and fit regression model\n",
    "    model=algoritm()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # predict\n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    # evaluate\n",
    "    train_accuracy = roc_auc_score(y_train, train_preds)\n",
    "    test_accuracy = roc_auc_score(y_test, test_preds)\n",
    "    #report = classification_report(y_test, test_preds)\n",
    "\n",
    "    print(str(algoritm))\n",
    "    print(\"------------------------\")\n",
    "    print(f\"Training Accuracy: {(train_accuracy * 100):.4}%\")\n",
    "    print(f\"Test Accuracy:     {(test_accuracy * 100):.4}%\")\n",
    "\n",
    "    # store accuracy in a new dataframe\n",
    "    score_logreg = [algoritm, train_accuracy, test_accuracy]\n",
    "    models = pd.DataFrame([score_logreg])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all classifiers that I will run for base models \n",
    "algoritms=[LogisticRegression,KNeighborsClassifier,RandomForestClassifier,XGBClassifier,svm.SVC]\n",
    "\n",
    "#running each model and print accuracy scores\n",
    "for algoritm in algoritms:\n",
    "    ml_func (algoritm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Grid Search for Best Hyper-Parameter\n",
    "\n",
    "In the above report we see the best training accuracy is from Random Forest Classifier. On the other hand the best test accuracy is from XGBoost Classifier. I would like to optimize these two model with grid search of multiple parameter values. Grid earch will help me to figure our best parameters to pass to the model to get the most accurate result. I will create a function for grid search named best_param. It will take parameter values and the classifer and print our the best parameter combinations. **I will only run Random Forest and XGBoost models for the rest of the project since they are the best two.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A general function for grdi search\n",
    "def grid_src(classifier, param_grid):\n",
    "    param_grid=param_grid\n",
    "    # instantiate the tuned random forest\n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=3, n_jobs=-1)\n",
    "\n",
    "    # train the tuned random forest\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # print best estimator parameters found during the grid search\n",
    "    print((str(classifier) + 'Best Parameters'))\n",
    "    print(\"------------------------\")\n",
    "    print(grid_search.best_params_)\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search for best parameters of RandomForestClassifier\n",
    "param_grid_rf = {'n_estimators': [10, 80, 100],\n",
    "                  'criterion': ['gini', 'entropy'],         \n",
    "                  'max_depth': [10], \n",
    "                  'min_samples_split': [2, 3, 4]\n",
    "                 }\n",
    "\n",
    "rf_params=grid_src(RandomForestClassifier(),param_grid_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search for best parameters of XGBClassifier\n",
    "param_grid_xg = {'n_estimators': [100],\n",
    "              'learning_rate': [0.05, 0.1], \n",
    "              'max_depth': [3, 5, 10],\n",
    "              'colsample_bytree': [0.7, 1],\n",
    "              'gamma': [0.0, 0.1, 0.2]\n",
    "                }\n",
    "grid_src(XGBClassifier(), param_grid_xg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run models with their best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to train and evaluate a  model with given datasets \n",
    "#it also prints the accuracy scores \n",
    "def run_model(model, X_train, y_train,X_test, y_test ):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # predict\n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    # evaluate\n",
    "    train_accuracy = roc_auc_score(y_train, train_preds)\n",
    "    test_accuracy = roc_auc_score(y_test, test_preds)\n",
    "    report = classification_report(y_test, test_preds)\n",
    "\n",
    "    #print reports of the model accuracy\n",
    "    print('Model Scores')\n",
    "    print(\"------------------------\")\n",
    "    print(f\"Training Accuracy: {(train_accuracy * 100):.4}%\")\n",
    "    print(f\"Test Accuracy:     {(test_accuracy * 100):.4}%\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "    print('Classification Report : \\n', report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running RandomForestClassifier with best parameters\n",
    "rf_model=RandomForestClassifier(n_estimators=100, \n",
    "                                  criterion= 'gini', \n",
    "                                  max_depth= 10, \n",
    "                                  min_samples_split= 3)\n",
    "                               \n",
    "                               \n",
    "run_model(rf_model, X_train, y_train,X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy dropped because I set max_depth to 10. This is kind of cut off for the model to stop after that point. The result with defauul value is higher because it goes unlimited until all leaves are the purist level. But it will take too long for a big dataset. I will keep this parameter and try to improve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running XGBClassifier with best parameters\n",
    "xgb_model=XGBClassifier(colsample_bytree= 1, \n",
    "                        n_estimators= 100,\n",
    "                        gamma= 0.1,\n",
    "                        learning_rate=0.1,\n",
    "                        max_depth=5\n",
    "                        )\n",
    "                                                                 \n",
    "run_model(xgb_model, X_train, y_train,X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost definetely works better with the best parameters set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomforest classifier might be effected the skewness of the target. Our data is quite unbalanced. That skewness can be taken care by resampling the data via SMOTE. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Dealing with Unbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. Balancing Data via Oversampling with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# view previous class distribution\n",
    "print(target.value_counts()) \n",
    "\n",
    "# resample data ONLY using training data\n",
    "X_resampled, y_resampled = SMOTE().fit_sample(X_train, y_train) \n",
    "\n",
    "# view synthetic sample class distribution\n",
    "print(pd.Series(y_resampled).value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform  train-test-split over resampled data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running models with the balanced data with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running RainForest Model with resampled data\n",
    "run_model(rf_model, X_train, y_train,X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running XGBoost Model with resampled data\n",
    "run_model(xgb_model, X_train, y_train,X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wouw, the performence increased dramatically for both models. Having almost 100% accuracy is suspicious though. It is probably because of the synthetic data that SMOTE created. Since there are only small amount of instances for fraud class, it created too many of the same data. Model memorize that pattern and gives perfect result on the test set. Because, there is highly possible that same data points are also availble in the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Subsampling Data from the Original Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had a huge dataset at the beginning and I did random sampling to reduce the computational laod. But I have a lot more natural fraud data point in this dataset that I can use. Insted of creating syntetic data I will choose those pints and randomly choose the safe transaction data points to get less skewed sample for my models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the only types that fraud transaction occurs\n",
    "data2=data[data['type'].isin(['TRANSFER','CASH_OUT'])]\n",
    "#Slice data in to fraud and safe by isFraud values\n",
    "safe_2 = data2[data2['isFraud']==0]\n",
    "fraud_2 = data2[data2['isFraud']==1]\n",
    "#get 50000 random sample from the safe transactions \n",
    "safe_sample=safe_2.sample(n=50000, random_state=1)\n",
    "safe_sample=safe_sample.sort_index()\n",
    "safe_samplef=safe_sample.reset_index(drop=True)\n",
    "#combine all fraud observation and 50000 safe transaction data in to df3\n",
    "df3=pd.concat([safe_sample,fraud_2])\n",
    "df3.reset_index(drop=True)\n",
    "#drop name columns\n",
    "df3=df3.drop(['nameOrig', 'nameDest'], axis=1)\n",
    "#Binary-encoding of labelled data in 'type'\n",
    "df3.loc[df3.type == 'CASH_OUT', 'type'] = 1\n",
    "df3.loc[df3.type == 'TRANSFER', 'type'] = 0\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check class labels\n",
    "df3.isFraud.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the new dataset with totally natural data is ready for going in to our models. The proportion is still not 50% but good enough to train a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running models with subsampled natural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slide the target and features from the dataset\n",
    "features2=df3.drop('isFraud', axis=1)\n",
    "target2 =df3.isFraud\n",
    "# split the data into train and test\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(features2, target2, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running RandomForestClassifier with best parameters\n",
    "run_model(rf_model, X_train2, y_train2,X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running XGBClassifier with best parameters\n",
    "\n",
    "run_model(xgb_model, X_train2, y_train2,X_test2, y_test2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look much realistic. I can still use SMOTE on this new dataset and see how it effects the results now. But XGBoost model seems to be working clearly better in any set of data so far. Eventhough we have better proportion we still have unbalanced data. We can permofm oversampling on this new data to have more fraud data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Performing SMOTE on the New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# view previous class distribution\n",
    "print(target2.value_counts()) \n",
    "\n",
    "# resample data ONLY using training data\n",
    "X_resampled2, y_resampled2 = SMOTE().fit_sample(X_train2, y_train2) \n",
    "\n",
    "# view synthetic sample class distribution\n",
    "print(pd.Series(y_resampled2).value_counts()) \n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(features2, target2, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running models with subsampled and oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running RandomForestClassifier with best parameters\n",
    "run_model(rf_model, X_train2, y_train2,X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running XGBClassifier with best parameters\n",
    "run_model(xgb_model, X_train2, y_train2,X_test2, y_test2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost improved a little bit more bu t Random Forest accuracy decreased with this new data. I can say that Random Forest can not handling too many repeted data for the sake of balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines are extremely useful tools to write clean and manageable code for machine learning.Creating a model takes a many steps such as clean our data, transform it, potentially use feature selection, and then run a machine learning algorithm. Using pipelines, we can do all these steps in one go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load necessary libraries for ml pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "pipe = Pipeline([('scl', MinMaxScaler()),\n",
    "                ('pca', PCA(n_components=7)),\n",
    "                ('xgb', XGBClassifier())])\n",
    "\n",
    "# Create the grid parameter\n",
    "grid = [{'xgb__n_estimators': [100],\n",
    "         'xgb__learning_rate': [0.05, 0.1], \n",
    "         'xgb__max_depth': [3, 5, 10],\n",
    "         'xgb__colsample_bytree': [0.7, 1],\n",
    "         'xgb__gamma': [0.0, 0.1, 0.2]\n",
    "                }]\n",
    "                                 \n",
    "# Create the grid, with \"pipe\" as the estimator\n",
    "gridsearch = GridSearchCV(estimator=pipe,\n",
    "                  param_grid=grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=3)\n",
    "\n",
    "# Fit using grid search\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Best accuracy\n",
    "print('Best accuracy: %.3f' % gridsearch.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gridsearch.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check whick features are the most influencial ones for both model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the important features - based on Random Forest\n",
    "feat_importances = pd.Series(rf_model.feature_importances_, index=features.columns)\n",
    "ax.set_ylabel('features', size = 16);\n",
    "feat_importances.nlargest(10).sort_values().plot(kind='barh', figsize=(10,5))\n",
    "plt.xlabel('Relative Feature Importance for Random Forest');\n",
    "plt.title('Feature Importance Order', size = 16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the important features - based on XGBOOST\n",
    "from xgboost import plot_importance\n",
    "\n",
    "fig = plt.figure(figsize = (10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "colours = plt.cm.Set1(np.linspace(0, 1, 9))\n",
    "ax = plot_importance(xgb_model, height = 0.5, color = 'orange', grid = False, \\\n",
    "                     show_values = False, importance_type = 'cover', ax = ax);\n",
    "for axis in ['top','bottom','left','right']:\n",
    "            ax.spines[axis].set_linewidth(2)        \n",
    "ax.set_xlabel('Relative Feature Importance for XGBoost', size=12);\n",
    "ax.set_yticklabels(ax.get_yticklabels(), size = 12);\n",
    "ax.set_title('Feature Importance Order', size = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model gives different importance to the features. But oldbalanceOrg and newbalanceDest are the major indicators for both models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy results after iterations\n",
    "I created a model that can predict fraud transactions. I used XGBoost and RandomForest  classifiers in this model. \n",
    "\n",
    "      (Data & Parameters)                              (Accuracy)   XGBoost   RandomForest\n",
    "      **Iteration 1**                                                            \n",
    "    - Random Sample & default parameters                                87%       86%                **Iteration 2** \n",
    "    - Random Sample & best parameters                                   99.5%     84.3%\n",
    "      **Iteration 3**\n",
    "    - Balanced data with SMOTE & best parameters                        99.4%     98.7% \n",
    "      **Iteration 4**\n",
    "    - Random Safe trans. data and all Fraud data & best parameters      98.8%     99.6%\n",
    "      **Iteration 5**\n",
    "    - New data balanced with SMOTE & best parameters                    99%       92.1%\n",
    "\n",
    "\n",
    "Performence has increased after five iterations and finally reached to; \n",
    "### 99% accuracy with XGBoost Classifier and Balanced Data\n",
    "\n",
    "\n",
    "### Most Influential Features\n",
    "- Most important features are senders balance before the transaction (oldBalanceOrig) and receivers balance after the transaction (newBalanceDest). \n",
    "\n",
    "\n",
    "### EDA Findings\n",
    "\n",
    "- Eventhough safe transactions slows down in 3rd and 4th day and after 16th day of the month, fraud transactions happens at a steady pace. Especially in the second half of the month there are much less safe transactions but number of fraud transactions does not decrease at all. \n",
    "- Fraud proportion over all transactions is 0.01% while the fraud amount proportion is 0.1%\n",
    "- There is some sort of seasonality in the number of transaction every 24 hours.Fraud transactions does not show that significant pattern. They happen every hour almost in the same frequency.\n",
    "- There are more fraud transactions in low amounts and less in high amount. This distribution does not change much.\n",
    "- Fraud transaction happens in a large range such as 119 dolars to 10M dolars. Most of the fraud transactions are of Lesser amount. But in 1M there is an interesting increase similar to safe transactions. \n",
    "- There are 16 fake fraud cases  with '0' amount.\n",
    "- Fraud activities only happens with TRANSFER and CASH_OUT transactions. DEBIT usage is very safe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.Future Work\n",
    "\n",
    "- I would like reindex this dataset with timestapms and analyze it as time series. I believe I would find some seosonality on both fraud transaction frequency and amount as well. \n",
    "- It migth also help to predict fraud transactions, that way companies could be axtra cautios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
